{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwintenkorang/PSO-A2C-Lnet/blob/main/Cleaned_Up_Code_for_PSO_A2C_LNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z4O044hHiQCX",
        "outputId": "998523c8-6aef-4955-f618-b6bfcbe0f8b5"
      },
      "source": [
        "#Import the data from Google Drive\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5XycCXvjJoq"
      },
      "outputs": [],
      "source": [
        "#'/content/drive/My Drive/Data for PSO-A2C-LNet/your_file.csv'\n",
        "file_path_train = '/content/drive/MyDrive/Data for PSO-A2C-LNet/Panama Dataset/train_dataframes.xlsx'\n",
        "file_path_test = '/content/drive/MyDrive/Data for PSO-A2C-LNet/Panama Dataset/test_dataframes.xlsx'\n",
        "import pandas as pd\n",
        "data = pd.read_excel(file_path_train)\n",
        "data_y = pd.read_excel(file_path_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2m1OZ6byMeq"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DISHvU5dXimT"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "data = pd.read_excel(io.BytesIO(uploaded['train_dataframes (1).xlsx']))\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zGziXBQp0fr"
      },
      "outputs": [],
      "source": [
        "#Installations\n",
        "!pip install pyswarm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYp6EJRMkOIf"
      },
      "outputs": [],
      "source": [
        "#BEST ACCURACY SO FAR FOR THE PANAMA == 97.1903%\n",
        "#A2C-LNet\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, Dropout, LSTM, Dense, Bidirectional, Activation, Multiply\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "# Data Preprocessing\n",
        "features = data.drop(columns=['datetime', 'DEMAND']).values\n",
        "target = data['DEMAND'].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "window_size = 4\n",
        "X, y = [], []\n",
        "for i in range(len(features) - window_size + 1):\n",
        "    X.append(features[i:i + window_size])\n",
        "    y.append(target[i + window_size - 1])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Reshape X for the Conv1D layer\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
        "\n",
        "# Step 2: Splitting the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "input_shape = (window_size, features.shape[1])\n",
        "\n",
        "input_layer = Input(shape=input_shape)\n",
        "conv_layer = Conv1D(filters=64, kernel_size=4, activation='relu')(input_layer)\n",
        "dropout1_layer = Dropout(0.2)(conv_layer)\n",
        "lstm_layer = Bidirectional(LSTM(128, activation='tanh'))(dropout1_layer)\n",
        "dropout2_layer = Dropout(0.2)(lstm_layer)\n",
        "\n",
        "# Attention Module\n",
        "attention_layer = Dense(1, activation='tanh')(dropout2_layer)\n",
        "attention_layer = Activation('softmax')(attention_layer)\n",
        "attention_mul_layer = Multiply()([dropout2_layer, attention_layer])\n",
        "\n",
        "output_layer = Dense(1, activation='linear')(attention_mul_layer)  # Output layer with linear activation for regression\n",
        "\n",
        "# Model Compilation\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer=Adam(), loss='mean_absolute_percentage_error')\n",
        "model.summary()\n",
        "\n",
        "# Train the Model\n",
        "model.fit(X_train, y_train, epochs=3500, batch_size=16)\n",
        "\n",
        "# Evaluate the Model\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "# Check if the arrays have the same shape\n",
        "if y_test.shape != y_pred.shape:\n",
        "    raise ValueError(\"Shape mismatch between y_test and y_pred\")\n",
        "\n",
        "# Calculate R^2\n",
        "def r_squared(y_true, y_pred):\n",
        "    residual = tf.reduce_sum(tf.square(tf.subtract(y_true, y_pred)))\n",
        "    total = tf.reduce_sum(tf.square(tf.subtract(y_true, tf.reduce_mean(y_true))))\n",
        "    r2 = 1 - tf.divide(residual, total)\n",
        "    return r2\n",
        "\n",
        "r2 = r_squared(y_test, y_pred)\n",
        "print(\"R^2 Score:\", r2.numpy())\n",
        "\n",
        "# MAE\n",
        "mae = tf.reduce_mean(tf.abs(y_test - y_pred))\n",
        "print(\"Mean Absolute Error (MAE):\", mae.numpy())\n",
        "\n",
        "# MAPE\n",
        "mape = tf.reduce_mean(tf.abs((y_test - y_pred) / y_test)) * 100\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", mape.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm9Pct8EyX6h"
      },
      "outputs": [],
      "source": [
        "#BEST ACCURACY SO FAR FOR THE PANAMA == 98.0624%\n",
        "#Adding PSO to the A2C-LNet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, Dropout, LSTM, Dense, Bidirectional, Activation, Multiply\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from pyswarm import pso\n",
        "\n",
        "# Define the objective function for PSO\n",
        "def optimize_hyperparameters(params):\n",
        "    learning_rate, batch_size, num_epochs, weight_init, loss_metric = params\n",
        "\n",
        "    # Data Preprocessing (Assuming 'data' is loaded previously)\n",
        "    features = data.drop(columns=['datetime', 'DEMAND']).values\n",
        "    target = data['DEMAND'].values\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = MinMaxScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    window_size = 4\n",
        "    X, y = [], []\n",
        "    for i in range(len(features) - window_size + 1):\n",
        "        X.append(features[i:i + window_size])\n",
        "        y.append(target[i + window_size - 1])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Reshape X for the Conv1D layer\n",
        "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
        "\n",
        "    # Splitting the Data\n",
        "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)  # Training data\n",
        "\n",
        "    # Assuming 'data_y' contains the testing dataset\n",
        "    features_y = data_y.drop(columns=['datetime', 'DEMAND']).values\n",
        "    target_y = data_y['DEMAND'].values\n",
        "\n",
        "    # Normalize features for the testing dataset\n",
        "    features_y = scaler.transform(features_y)\n",
        "\n",
        "    X_y, y_y = [], []\n",
        "    for i in range(len(features_y) - window_size + 1):\n",
        "        X_y.append(features_y[i:i + window_size])\n",
        "        y_y.append(target_y[i + window_size - 1])\n",
        "\n",
        "    X_y = np.array(X_y)\n",
        "    y_y = np.array(y_y)\n",
        "\n",
        "    # Reshape X for the Conv1D layer for the testing dataset\n",
        "    X_y = X_y.reshape(X_y.shape[0], X_y.shape[1], X_y.shape[2])\n",
        "\n",
        "    # Define initializer\n",
        "    initializer = None\n",
        "\n",
        "    input_shape = (window_size, features.shape[1])\n",
        "\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    conv_layer = Conv1D(filters=64, kernel_size=4, activation='relu')(input_layer)\n",
        "    dropout1_layer = Dropout(0.2)(conv_layer)\n",
        "    lstm_layer = Bidirectional(LSTM(128, activation='tanh'))(dropout1_layer)\n",
        "    dropout2_layer = Dropout(0.2)(lstm_layer)\n",
        "\n",
        "    # Create a custom initializer for the dense layers\n",
        "    dense_initializer = tf.keras.initializers.get(initializer)\n",
        "\n",
        "    # Use the custom initializer in the Dense layers\n",
        "    # Attention Module\n",
        "    attention_layer = Dense(1, activation='tanh', kernel_initializer=dense_initializer)(dropout2_layer)\n",
        "    attention_layer = Activation('softmax')(attention_layer)\n",
        "    attention_mul_layer = Multiply()([dropout2_layer, attention_layer])\n",
        "\n",
        "    output_layer = Dense(1, activation='linear', kernel_initializer=dense_initializer)(attention_mul_layer)\n",
        "\n",
        "    if weight_init == 0:\n",
        "        initializer = 'glorot_uniform'  # Xavier initialization\n",
        "    elif weight_init == 1:\n",
        "        initializer = 'he_normal'  # He initialization\n",
        "    else:\n",
        "        initializer = 'random_normal'  # Random initialization\n",
        "\n",
        "    loss_metric = round(loss_metric)\n",
        "    weight_init = round(weight_init)\n",
        "\n",
        "    if loss_metric == 0:\n",
        "      loss_metric = 'mean_squared_error'\n",
        "    elif loss_metric == 1:\n",
        "      loss_metric = 'binary_crossentropy'\n",
        "    else:\n",
        "      loss_metric = 'mean_absolute_percentage_error'\n",
        "\n",
        "    num_epochs = round(num_epochs+0.5)\n",
        "    batch_size = round(batch_size+0.5)\n",
        "\n",
        "    # Model Compilation\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer=Adam(learning_rate), loss=loss_metric, metrics=['mape'])\n",
        "\n",
        "    # Train the Model\n",
        "    model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    # Evaluate the Model on the testing dataset\n",
        "    y_pred = model.predict(X_y).flatten()\n",
        "\n",
        "    # Calculate MAPE for the testing dataset\n",
        "    mape = tf.reduce_mean(tf.abs((y_y - y_pred) / y_y)) * 100\n",
        "\n",
        "    return mape.numpy()\n",
        "\n",
        "# Parameter ranges for PSO\n",
        "lb = [0.001, 1, 100, 0, 0]  # Lower bounds for learning_rate, batch_size, num_epochs, weight_init, loss_metric\n",
        "ub = [0.1, 128, 5000, 2, 2]  # Upper bounds for learning_rate, batch_size, num_epochs, weight_init, loss_metric\n",
        "\n",
        "# Perform PSO optimization\n",
        "best_params, _ = pso(optimize_hyperparameters, lb, ub, swarmsize=1200, maxiter=1000)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "learning_rate, batch_size, num_epochs, weight_init, loss_metric = best_params\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(\"Learning Rate:\", learning_rate)\n",
        "print(\"Batch Size:\", int(batch_size))\n",
        "print(\"Number of Epochs:\", int(num_epochs))\n",
        "if weight_init == 0:\n",
        "    print(\"Weight Initialization: Xavier\")\n",
        "elif weight_init == 1:\n",
        "    print(\"Weight Initialization: He\")\n",
        "else:\n",
        "    print(\"Weight Initialization: Random\")\n",
        "if loss_metric == 0:\n",
        "    print(\"Loss Metric: Mean Squared Error (MSE)\")\n",
        "elif loss_metric == 1:\n",
        "    print(\"Loss Metric: Cross-Entropy\")\n",
        "else:\n",
        "    print(\"Loss Metric: Mean Absolute Percentage Error (MAPE)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
